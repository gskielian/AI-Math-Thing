Learning Curve Optimization
===========================

Join us in the quest to create algorithms for developing curricula which optimize the learning curve!

## Roadmap

Markov Chains mod, doesn't add up 1, leaky transitions also kept, but only interested in the ones which hold water.
These will be the groups (approximate groups which feedback into themselves, although not necessarily in a fixed sequence, hence *groups not loops*)

Groups are interesting as opposed to the simple loops as well because of they have freedom on the permutation of operations (loops are to groups as circle is to conic section)

### Model the mind as a set of states and a set of transitions

Self-explanatory, and each state is largely defined by the relative probabilities

### Group closure is weighted

Paths that are traversed before are easier to traverse a second time, i.e. priming.  Once the conduits are primed it is more probable that they will be traversed again.

Organisms tend to gravitate towards groups.

### Weight different sized groups -- not too small (they quickly become annoying and lack the higher-order boost)

http://www.pnas.org/content/early/2012/01/13/1109863109.full.pdf+html?with-ds=yes

we are more interested in the paths which repeat themselves less often this is to the learning curve affects observed
at the UCLA Learning & Forgetting Labs [link](http://www.wired.com/geekdad/2012/01/everything-about-learning/)

We look here a priori for prospective paths.


larger groups are less susceptible to vestige, as the diversity within states and long delay between global repetitions act as a low pass for noise.

